<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>A Glitch in the Algorithm</title>
  <style>
    body {
      background-color: #fdf6e3;
      color: #073642;
      font-family: 'Courier New', monospace;
      padding: 30px;
      max-width: 800px;
      margin: auto;
    }
    
    nav {
      text-align: center;
      margin-bottom: 20px;
    }
    nav img {
      width: 50px;
      cursor: pointer;
      margin: 0 5px;
      transition: transform 0.2s ease;
    }
    nav img:hover {
      transform: scale(1.1);
    }

    section {
      display: none;
      opacity: 0;
      transition: opacity 0.4s ease-in-out;
    }
    section.active {
      display: block;
      opacity: 1;
    }
    h1, h2 {
      color: #b58900;
    }
    .footer {
      text-align: center;
      margin-top: 40px;
      font-size: 0.9em;
      color: #586e75;
    }
    .quote {
      background-color: #eee8d5;
      padding: 10px;
      border-left: 4px solid #b58900;
      margin: 20px 0;
      font-style: italic;
    }
  </style>
</head>
<body>
  <h1>A Glitch in the Algorithm</h1>
  <p><em>You ever feel like the algorithm hates you? You're not imagining it...</em></p>
  
  <nav aria-label="Zine navigation">
    <img src="pngtree-black-computer-keyboard-key-number-1-3d-object-png-image_11262727.png" alt="Page 1" onclick="showPage(0)" style="width:50px; cursor:pointer; margin: 0 5px;">
    <img src="pngtree-black-computer-keyboard-key-number-2-3d-graphic-photo-png-image_14393572.png" alt="Page 2" onclick="showPage(1)" style="width:50px; cursor:pointer; margin: 0 5px;">
    <img src="pngtree-black-computer-keyboard-key-number-3-3d-close-up-tech-photo-png-image_14393573.png" alt="Page 3" onclick="showPage(2)" style="width:50px; cursor:pointer; margin: 0 5px;">
    <img src="pngtree-black-computer-keyboard-key-number-4-3d-macro-png-image_10281118.png" alt="Page 4" onclick="showPage(3)" style="width:50px; cursor:pointer; margin: 0 5px;">
    <img src="pngtree-black-computer-keyboard-key-number-5-3d-element-photo-png-image_13420717.png" alt="Page 5" onclick="showPage(4)" style="width:50px; cursor:pointer; margin: 0 5px;">
  </nav>

<section class="active">
  <h2>The Myth of Neutrality</h2>
  <p>Turns out “smart” tech might be smart like a 1950s hiring manager… Algorithms don’t live in a vacuum — they reflect the values, biases, and blind spots of the people and institutions who build them.</p>
  <p>In <strong>Algorithms of Oppression</strong>, Safiya Noble demonstrates how search engines like Google surface dehumanizing and stereotypical content when users search for terms like “Black girls.” These results aren’t accidental. They stem from ranking algorithms that prioritize engagement and profit over ethics or accuracy. Noble argues this isn't a glitch in the system — it's the system working as designed within capitalist and discriminatory frameworks.</p>
  <p>Meanwhile, <strong>Hazel Henderson</strong>, in “Progress or Collective Insanity,” warns against our overreliance on the idea that technology equals progress. She points out how innovations are often developed within narrow economic structures that serve existing power hierarchies. Instead of broadening access and opportunity, digital tools often reinforce societal inequalities when left unchecked.</p>

  <div class="quote">“Search is a mirror of users’ beliefs and that society still holds a variety of sexist ideas about women.”<br>— Safiya Umoja Noble</div>

  <p>Together, these works dismantle the myth that technology is neutral. From racist autocomplete suggestions to discriminatory image recognition, the evidence shows that digital systems mirror the very real inequalities of the offline world. These are not isolated bugs — they’re symptoms of deeper issues in how data is collected, categorized, and monetized.</p>
  <p>This isn’t just a question of tech literacy — it’s a political and social issue. Understanding the myth of neutrality is the first step in demanding more accountability and justice from the algorithms that increasingly govern our lives.</p>
</section>

  <section>
    <h2>Bias in the Machine</h2>
    <p>Facial recognition sounds futuristic — until it can’t recognize your face. The <strong>Gender Shades</strong> study by Joy Buolamwini and Timnit Gebru put a spotlight on commercial AI systems that routinely misidentify darker-skinned individuals, especially women.</p>
    <p>These AI tools — marketed as objective, data-driven, and scalable — performed best on lighter-skinned men, with error rates as low as 0.8%. But for darker-skinned women? <em>The chance of misclassification for darker-skinned females in some classifiers was over 34%.</em> That’s not just a bug — that’s systemic.</p>
    <p>So, how does this happen? A major factor is the training data: many of these models are trained on skewed image sets that underrepresent darker skin tones, women, and people with intersectional identities. When these biases go unchecked, the technology doesn't just mirror inequality — it reinforces it at scale.</p>
    <p>This matters. Facial recognition is already being used in policing, hiring, and housing. Imagine being flagged as a suspect, denied a job, or misclassified by a healthcare system — all because the algorithm can’t “see” you accurately.</p>
    <p>Buolamwini's thesis further emphasizes the lack of accountability in these technologies. Developers often overlook the social consequences of their tools, especially when their teams lack racial and gender diversity.</p>
    <p>The core issue isn’t just the code — it’s the worldview baked into it. If the people designing systems don’t reflect the full spectrum of society, their creations won’t either.</p>
  </section>

  <section>
    <h2>Opinion Machines</h2>
    <p>Scroll long enough and you’ll find yourself wondering: is this what I think — or what the algorithm thinks I should think? Platforms like YouTube, TikTok, and Instagram don’t just serve content — they serve narratives. And those narratives are shaped by opaque algorithms whose logic we rarely see.</p>
    <p><strong>Natali Helberger</strong>, in “The Political Power of Platforms,” argues that algorithmic curation is far from neutral. These systems don’t simply filter what we see — they actively shape our political and emotional environments. And attempts to regulate them? Often more concerned with national interest or profit than user autonomy.</p>
    <p>Even well-meaning regulations can reinforce dominant ideologies, especially when platform power is left unchecked. The same companies that profit from our engagement also write the rules of what “appropriate” content looks like. That’s not moderation — that’s influence at scale.</p>
    <p>Imagine this: you're scrolling through a newsfeed, and every “suggested” post quietly pushes a specific worldview — not because it’s true, but because it keeps you scrolling. These aren't opinion-free zones; they're monetized echo chambers. The more predictable your reaction, the more profitable your clicks.</p>
  
    <div class="quote">“The source of the political power of platforms is their ability to wield opinion
power, whether it is that of their users or politicians, or their ability to influence public
discourse for their own purposes.”<br>— Natali Helberger</div>

    <label><input type="checkbox" disabled checked> I agree to be manipulated for ad revenue.</label>
    <p style="font-style: italic; color: #dc322f;">(Text glitching... please refresh your biases)</p>
  </section>

  <section>
    <h2>Can We Fix It?</h2>
    <p>It’s easy to feel stuck in the current system — but alternatives exist. Instead of accepting algorithmic bias as inevitable, we can push for more transparency, diverse datasets, ethical frameworks, and human-centered design.</p>
    <p><em>We can build again: federated platforms, ethical AI, and DIY tools are all ways to reclaim autonomy and challenge the dominant systems.</em></p>
  </section>

  <section>
    <h2>Take Action (or Just Poke Around)</h2>
    <p>Ready to explore more or take action?</p>
    <ul>
      <li><a href="https://ainowinstitute.org" target="_blank">AI Now Institute</a></li>
      <li><a href="https://neocities.org" target="_blank">Start your own retro site</a></li>
    </ul>
    <p><strong>Bonus:</strong> Click the cat that looks most hireable. (Just kidding. Or are we?)</p>
    <p style="color: #dc322f; animation: blink 1s step-start infinite;">[INSERT BLINKING PIXEL BUTTONS]</p>
  </section>


  <script>
    const sections = document.querySelectorAll('section');
    function showPage(index) {
      sections.forEach((section, i) => {
        section.classList.remove('active');
        if (i === index) {
          section.classList.add('active');
        }
      });
    }
    showPage(0);
  </script>
</body>
</html>
