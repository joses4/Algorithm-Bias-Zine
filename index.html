<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>A Glitch in the Algorithm</title>
  <style>
    body {
      background-color: #fdf6e3;
      color: #073642;
      font-family: 'Courier New', monospace;
      padding: 30px;
      max-width: 800px;
      margin: auto;
    }
    
    nav {
      text-align: center;
      margin-bottom: 20px;
    }
    nav img {
      width: 50px;
      cursor: pointer;
      margin: 0 5px;
      transition: transform 0.2s ease;
    }
    nav img:hover {
      transform: scale(1.1);
    }

    section {
      display: none;
      opacity: 0;
      transition: opacity 0.4s ease-in-out;
    }
    section.active {
      display: block;
      opacity: 1;
    }
    h1, h2 {
      color: #b58900;
    }
    .footer {
      text-align: center;
      margin-top: 40px;
      font-size: 0.9em;
      color: #586e75;
    }
    .quote {
      background-color: #eee8d5;
      padding: 10px;
      border-left: 4px solid #b58900;
      margin: 20px 0;
      font-style: italic;
    }

  .search-mockup {
    margin: 25px 0;
    background-color: #fefcf5;
    border: 1px solid #ccc;
    padding: 16px;
    border-radius: 10px;
    font-family: Arial, sans-serif;
    box-shadow: 2px 2px 10px rgba(0,0,0,0.1);
  }

.search-bar {
  display: flex;
  align-items: center;
  margin-bottom: 10px;
}

.google-logo {
  width: 40px;
  margin-right: 10px;
  vertical-align: middle;
  margin-top: 2px;
}

.search-bar input {
  width: 100%;
  padding: 10px;
  font-size: 16px;
  border: 1px solid #ddd;
  border-radius: 5px;
}

.autocomplete-box {
  margin-left: 32px;
  background: #fff;
  border: 1px solid #eee;
  border-radius: 5px;
  padding: 10px;
  color: #555;
}

.autocomplete-box p {
  margin: 2px 0;
  padding: 2px 6px;
}

.autocomplete-box .highlighted {
  background-color: #fdf6e3;
  font-weight: bold;
  color: #b58900;
}

.caption {
  margin-top: 10px;
  font-size: 0.85em;
  color: #586e75;
  font-style: italic;
}

.suggestions {
  opacity: 0;
  transform: translateY(10px);
  transition: all 0.4s ease;
  pointer-events: none;
}

.search-bar:focus-within + .suggestions,
.search-input:focus + .suggestions {
  opacity: 1;
  transform: translateY(0);
  pointer-events: auto;
}

.bias-diagram {
  display: flex;
  flex-wrap: wrap;
  align-items: flex-start;
  justify-content: center;
  margin-top: 40px;
  gap: 12px;
  font-family: Arial, sans-serif;
}

.bias-stage {
  display: flex;
  flex-direction: column;
  align-items: center;
  max-width: 140px;
  text-align: left;
}

.box {
  background-color: #eee8d5;
  color: #073642;
  padding: 8px 14px;
  border: 2px solid #b58900;
  border-radius: 6px;
  font-weight: bold;
  margin-bottom: 6px;
}

.bias-stage ul {
  list-style-type: disc;
  padding-left: 20px;
  margin: 0;
  font-size: 0.85em;
  color: #586e75;
}

.bias-stage li {
  margin-bottom: 4px;
}

.arrow {
  font-size: 2em;
  line-height: 1;
  color: #b58900;
  margin-top: 20px;
}

.bias-stage li {
  position: relative;
  cursor: help;
  margin-bottom: 4px;
  transition: background-color 0.3s ease;
}

.bias-stage li:hover {
  background-color: #fef9e7;
  border-radius: 4px;
}

.bias-stage li:hover::after {
  content: attr(data-tooltip);
  position: absolute;
  top: 100%;
  left: 0;
  background-color: #eee8d5;
  color: #073642;
  padding: 6px 10px;
  border: 1px solid #ccc;
  border-radius: 5px;
  white-space: nowrap;
  margin-top: 4px;
  font-size: 0.8em;
  box-shadow: 2px 2px 5px rgba(0, 0, 0, 0.1);
  z-index: 10;
  opacity: 0;
  animation: fadeIn 0.3s ease forwards;
}

.arrow {
  font-size: 2em;
  line-height: 1;
  color: #b58900;
  margin-top: 20px;
  animation: pulseArrow 1.2s ease-in-out infinite;
}

@keyframes fadeIn {
  to {
    opacity: 1;
  }
}

@keyframes pulseArrow {
  0%, 100% { transform: scale(1); }
  50% { transform: scale(1.3); }
}

.annotated-container {
      position: relative;
      max-width: 100%;
      margin-top: 20px;
    }

    .annotated-container img {
      width: 100%;
      height: auto;
      border: 1px solid #ccc;
      border-radius: 5px;
    }

    .dot {
      position: absolute;
      width: 16px;
      height: 16px;
      background-color: #dc322f;
      border-radius: 50%;
      cursor: pointer;
      border: 2px solid white;
      box-shadow: 0 0 4px rgba(0,0,0,0.3);
    }

    .tooltip {
      position: absolute;
      background-color: #eee8d5;
      color: #073642;
      padding: 8px 10px;
      font-size: 0.8em;
      border-radius: 5px;
      border: 1px solid #ccc;
      white-space: nowrap;
      display: none;
      z-index: 10;
    }

    .dot:hover + .tooltip {
      display: block;
    }

</style>
</head>
<body>
  <h1>A Glitch in the Algorithm</h1>
  <p><em>You ever feel like the algorithm hates you? You're not imagining it...</em></p>
  
  <nav aria-label="Zine navigation">
    <img src="pngtree-black-computer-keyboard-key-number-1-3d-object-png-image_11262727.png" alt="Page 1" onclick="showPage(0)" style="width:50px; cursor:pointer; margin: 0 5px;">
    <img src="pngtree-black-computer-keyboard-key-number-2-3d-graphic-photo-png-image_14393572.png" alt="Page 2" onclick="showPage(1)" style="width:50px; cursor:pointer; margin: 0 5px;">
    <img src="pngtree-black-computer-keyboard-key-number-3-3d-close-up-tech-photo-png-image_14393573.png" alt="Page 3" onclick="showPage(2)" style="width:50px; cursor:pointer; margin: 0 5px;">
    <img src="pngtree-black-computer-keyboard-key-number-4-3d-macro-png-image_10281118.png" alt="Page 4" onclick="showPage(3)" style="width:50px; cursor:pointer; margin: 0 5px;">
    <img src="pngtree-black-computer-keyboard-key-number-5-3d-element-photo-png-image_13420717.png" alt="Page 5" onclick="showPage(4)" style="width:50px; cursor:pointer; margin: 0 5px;">
  </nav>

<section class="active">
  <h2>The Myth of Neutrality</h2>
  <p>Turns out “smart” tech might be smart like a 1950s hiring manager… Algorithms don’t live in a vacuum — they reflect the values, biases, and blind spots of the people and institutions who build them.</p>
  <p>In <strong>Algorithms of Oppression</strong>, Safiya Noble demonstrates how search engines like Google surface dehumanizing and stereotypical content when users search for terms like “Black girls.” These results aren’t accidental. They stem from ranking algorithms that prioritize engagement and profit over ethics or accuracy. Noble argues this isn't a glitch in the system — it's the system working as designed within capitalist and discriminatory frameworks.</p>
  
  <div class="search-bar">
  <img src="Google_2015_logo.svg.png" class="google-logo" alt="Google logo" />
  <input class="search-input" type="text" value="Black girls" />
</div>

<div class="suggestions">
  <div>black girls are loud</div>
  <div>black girls attitude</div>
  <div>black girls fighting</div>
  <div><strong style="color: #b58900;">black girls...</strong></div>
</div>
  <p class="caption">
    This was once a real autocomplete result. Algorithms don’t just reflect society — they reinforce it.
  </p>
</div>
  
  <p>Meanwhile, <strong>Hazel Henderson</strong>, in “Progress or Collective Insanity,” warns against our overreliance on the idea that technology equals progress. She points out how innovations are often developed within narrow economic structures that serve existing power hierarchies. Instead of broadening access and opportunity, digital tools often reinforce societal inequalities when left unchecked.</p>

  <div class="quote">“Search is a mirror of users’ beliefs and that society still holds a variety of sexist ideas about women.”<br>— Safiya Umoja Noble</div>

  <p>Together, these works dismantle the myth that technology is neutral. From racist autocomplete suggestions to discriminatory image recognition, the evidence shows that digital systems mirror the very real inequalities of the offline world. These are not isolated bugs — they’re symptoms of deeper issues in how data is collected, categorized, and monetized.</p>
  <p>This isn’t just a question of tech literacy — it’s a political and social issue. Understanding the myth of neutrality is the first step in demanding more accountability and justice from the algorithms that increasingly govern our lives.</p>

  <div class="bias-diagram">
  <div class="bias-stage">
    <div class="box">World</div>
  </div>
  <div class="arrow">→</div>
  <div class="bias-stage">
    <div class="box">Data</div>
    <ul>
      <li data-tooltip="Bias from outdated beliefs and practices still embedded in data.">Historical Bias</li>
      <li data-tooltip="Over- or under-representation of groups.">Representation Bias</li>
      <li data-tooltip="Inaccurate or flawed measurement methods.">Measurement Bias</li>
      <li data-tooltip="Bias from time-lagged or outdated data.">Temporal Bias</li>
      <li data-tooltip="Missing key variables that explain disparities.">Omitted Variable Bias</li>
    </ul>
  </div>
  <div class="arrow">→</div>
  <div class="bias-stage">
    <div class="box">AI/ML</div>
    <ul>
      <li data-tooltip="The model reflects and amplifies bias.">Algorithmic Bias</li>
      <li data-tooltip="Bias from how performance is evaluated.">Evaluation Bias</li>
      <li data-tooltip="Combining groups that shouldn't be combined.">Aggregation Bias</li>
      <li data-tooltip="Bias toward popular results.">Popularity Bias</li>
      <li data-tooltip="Bias in how results are ranked.">Ranking Bias</li>
    </ul>
  </div>
  <div class="arrow">→</div>
  <div class="bias-stage">
    <div class="box">Human Review</div>
    <ul>
      <li data-tooltip="Bias in how people interpret model output.">Behavioral Bias</li>
      <li data-tooltip="Bias from how data is presented to users.">Presentation Bias</li>
      <li data-tooltip="Bias in what content gets created or shared.">Content Production Bias</li>
      <li data-tooltip="Social stereotypes shaping results.">Social Bias</li>
    </ul>
  </div>
  <div class="arrow">→</div>
  <div class="bias-stage">
    <div class="box">Actions</div>
  </div>
</div>


  
  <div style="text-align: center; margin-top: 20px;">
    <img src="BIAS-in-AI.jpg" alt="Diagram showing types of algorithmic bias" style="width: 75%; max-width: 500px; height: auto; border: 1px solid #ccc; border-radius: 5px;">
    <p style="font-size: 0.8em; color: #586e75; margin-top: 8px;">
    <em>Biases creep in from the real world, through data, and all the way into algorithmic decisions.</em>
    </p>
    <p style="font-size: 0.8em; color: #586e75; margin-top: 8px;">
    <em>Graphic originally featured by <a href="https://ubisglobal.com/blog/addressing-bias-in-ai-advancing-gender-race-and-age-equality/" target="_blank">UBIS</a>, further discussing types of algorithmic bias.</em>
    </p>
  </div>
</section>

  <section>
    <h2>Bias in the Machine</h2>
    <p>Facial recognition sounds futuristic — until it can’t recognize your face. The <strong>Gender Shades</strong> study by Joy Buolamwini and Timnit Gebru put a spotlight on commercial AI systems that routinely misidentify darker-skinned individuals, especially women.</p>
    <p>These AI tools — marketed as objective, data-driven, and scalable — performed best on lighter-skinned men, with error rates as low as 0.8%. But for darker-skinned women? <em>The chance of misclassification for darker-skinned females in some classifiers was over 34%.</em> That’s not just a bug — that’s systemic.</p>

    <div class="annotated-container">
      <img src="GenderShades_gs04.webp" alt="Classifier performance by skin tone and gender" />

      <span class="dot" style="top: 22%; left: 20%;"></span>
      <div class="tooltip" style="top: 10%; left: 25%;">Microsoft's classifier was 20.8% less accurate for darker-skinned women.</div>

      <span class="dot" style="top: 30%; left: 20%;"></span>
      <div class="tooltip" style="top: 27%; left: 25%;">FACE++ showed the widest accuracy gap: 33.8% difference between groups.</div>

      <span class="dot" style="top: 39%; left: 20%;"></span>
      <div class="tooltip" style="top: 44%; left: 25%;">IBM's tool had the lowest accuracy for darker-skinned women — just 65.3%.</div>
    </div>
  <p style="font-size: 0.8em; color: #586e75; margin-top: 10px;">
    <em>Adapted from Buolamwini & Gebru’s <strong>Gender Shades</strong> study (2018). Accuracy gaps in facial recognition systems reveal systemic algorithmic bias.</em>
  </p>
</div>
    
    <p>So, how does this happen? A major factor is the training data: many of these models are trained on skewed image sets that underrepresent darker skin tones, women, and people with intersectional identities. When these biases go unchecked, the technology doesn't just mirror inequality — it reinforces it at scale.</p>
    <p>This matters. Facial recognition is already being used in policing, hiring, and housing. Imagine being flagged as a suspect, denied a job, or misclassified by a healthcare system — all because the algorithm can’t “see” you accurately.</p>
    <p>Buolamwini's thesis further emphasizes the lack of accountability in these technologies. Developers often overlook the social consequences of their tools, especially when their teams lack racial and gender diversity.</p>
    <p>The core issue isn’t just the code — it’s the worldview baked into it. If the people designing systems don’t reflect the full spectrum of society, their creations won’t either.</p>
  </section>

  <section>
    <h2>Opinion Machines</h2>
    <p>Scroll long enough and you’ll find yourself wondering: is this what I think — or what the algorithm thinks I should think? Platforms like YouTube, TikTok, and Instagram don’t just serve content — they serve narratives. And those narratives are shaped by opaque algorithms whose logic we rarely see.</p>
    <p><strong>Natali Helberger</strong>, in “The Political Power of Platforms,” argues that algorithmic curation is far from neutral. These systems don’t simply filter what we see — they actively shape our political and emotional environments. And attempts to regulate them? Often more concerned with national interest or profit than user autonomy.</p>
    <p>Even well-meaning regulations can reinforce dominant ideologies, especially when platform power is left unchecked. The same companies that profit from our engagement also write the rules of what “appropriate” content looks like. That’s not moderation — that’s influence at scale.</p>
    <p>Imagine this: you're scrolling through a newsfeed, and every “suggested” post quietly pushes a specific worldview — not because it’s true, but because it keeps you scrolling. These aren't opinion-free zones; they're monetized echo chambers. The more predictable your reaction, the more profitable your clicks.</p>
  
    <div class="quote">“The source of the political power of platforms is their ability to wield opinion
      power, whether it is that of their users or politicians, or their ability to influence public
      discourse for their own purposes.”<br>— Natali Helberger</div>

    <label><input type="checkbox" disabled checked> I agree to be manipulated for ad revenue.</label>
    <p style="font-style: italic; color: #dc322f;">(Text glitching... please refresh your biases)</p>
  </section>

  <section>
    <h2>Can We Fix It?</h2>
    <p>It’s easy to feel stuck in the system. Like we’re just background noise in the algorithm’s master plan. But that doesn’t mean there’s no way out.</p>
    <p>The studies we’ve explored show the scope of the problem — from biased datasets to manipulative platforms. But the story doesn’t end there. Researchers like Joy Buolamwini and movements for algorithmic justice point us toward something better: accountability, transparency, and rethinking who gets to shape the digital world.</p>
    <p>One critical piece is community oversight. Imagine if the people affected by these systems were at the table — not just as test cases, but as co-designers. That means interdisciplinary teams, public audits, and resisting the idea that “data is truth” without context.</p>
    <p>Another piece? Better data. Diverse, representative datasets aren’t a silver bullet, but they’re a start. If an AI can’t recognize your face, it shouldn’t be used in policing or hiring. Period.</p>
    <p>And don’t forget: tech isn’t magic. It’s built by people — which means we can rebuild it, too. Smaller, open-source tools. Federated platforms. Local, transparent systems that don’t need to sell your attention to survive.</p>
    <p>Change won’t come from tweaking a few lines of code. It’ll come from rethinking the whole system — and who it’s supposed to serve.</p>
  </section>

  <section>
    <h2>Take Action (or Just Poke Around)</h2>
      <p>So now you’ve made it through the glitchy guts of algorithmic bias — what next?</p>
      <p>Start with awareness: keep asking questions like, “Who built this system?” and “Who benefits from it?” Bias isn’t always obvious, and the people affected the most often aren’t in the room when decisions are made. Learning to recognize bias is one step toward challenging it.</p>
      <p>Want to go deeper? Groups like the <strong>AI Now Institute</strong> and <strong>Algorithmic Justice League</strong> are pushing for ethical, transparent AI. You don’t need to be a programmer to care — policymakers, activists, designers, and everyday users all play a role.</p>
      <p>Maybe you’re not here to launch a movement — that’s okay. Even small things make a difference. Support creators and platforms that value privacy. Read beyond the algorithm. Talk to friends about what you’ve learned. Your curiosity is already resistance.</p>
      <p>Make art. Make noise. Build better tech. Or just… don’t feed the algo today.</p>
  </section>

  </section>


  <script>
    const sections = document.querySelectorAll('section');
    function showPage(index) {
      sections.forEach((section, i) => {
        section.classList.remove('active');
        if (i === index) {
          section.classList.add('active');
        }
      });
    }
    showPage(0);
  </script>
</body>
</html>
